{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation Using an LSTM Model in PyTorch**\n",
        "---"
      ],
      "metadata": {
        "id": "BJzPe85Xvur8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        ">This project implements a Long Short-Term Memory (LSTM) neural network using PyTorch to generate text based on an input sequence. The dataset consists of a sample document discussing bioinformatics, which is tokenized and numerically encoded before being used for training. The model learns to predict the next word in a sequence, allowing it to generate coherent sentences. The training process uses an embedding layer to convert words into vector representations, followed by an LSTM layer that captures contextual dependencies. After training, the model can generate new text by predicting the most likely next word given an input phrase."
      ],
      "metadata": {
        "id": "ATGk6Lluvx1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Importing Libraries and Setting Up the Environment**\n",
        "- **PyTorch** is used for building and training the LSTM model.\n",
        "- **NLTK (Natural Language Toolkit)** provides tools for tokenizing text.\n",
        "- **Counter** helps in creating a vocabulary based on word frequency.\n",
        "- **NumPy** is used for numerical operations.\n",
        "- **Time** is used for adding a delay in text generation."
      ],
      "metadata": {
        "id": "6iWEcjMGv2zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G73J4mRicgp",
        "outputId": "621e6174-bd3d-461e-9d62-f3fbba85c182"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "udLs8ojkiexB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Downloading Required NLTK Tokenizer**\n",
        "- Downloads the necessary NLTK tokenizer for processing text."
      ],
      "metadata": {
        "id": "7CF1wgx0wHyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knRzqJ7ui7Vi",
        "outputId": "17aa3753-abac-411f-ea01-4fd5708dd9da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Setting Up GPU/CPU for Computation**\n",
        "- The code checks if a GPU is available for faster computation; otherwise, it defaults to the CPU."
      ],
      "metadata": {
        "id": "MotniimVwNRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_Z05dQSrvzM",
        "outputId": "458bcfd6-fd42-4b8a-c182-508f3e99269b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Tokenizing and Creating a Vocabulary**\n",
        "- The text is tokenized into words.\n",
        "- A vocabulary dictionary is created, mapping each unique word to a numerical index.\n",
        "- `\"<UNK>\"` represents unknown words that are not in the vocabulary."
      ],
      "metadata": {
        "id": "fjcmUAoDwTZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YGt3QYci0dBP"
      },
      "outputs": [],
      "source": [
        "document = \"\"\"Introduction to Bioinformatics\n",
        "\n",
        "Bioinformatics is an interdisciplinary field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data. It plays a crucial role in modern life sciences, enabling researchers to understand complex biological systems, identify disease biomarkers, and develop personalized medicine approaches. With the increasing availability of high-throughput sequencing technologies, bioinformatics has become essential for processing large-scale genomic, transcriptomic, and proteomic data.\n",
        "\n",
        "Key Areas of Bioinformatics\n",
        "\n",
        "Genomics: This involves analyzing entire genomes to identify genes, regulatory elements, and variations. Techniques such as whole-genome sequencing (WGS) and genome-wide association studies (GWAS) help in identifying genetic mutations linked to diseases and evolutionary traits.\n",
        "\n",
        "Transcriptomics: This focuses on studying gene expression patterns using RNA sequencing (RNA-seq) or microarrays. It helps in understanding how genes are regulated in different conditions, such as disease states versus normal tissues.\n",
        "\n",
        "Proteomics: Bioinformatics tools analyze protein sequences, structures, and interactions. Mass spectrometry data processing, protein structure prediction, and functional annotation are key aspects of proteomics research.\n",
        "\n",
        "Metagenomics: This involves analyzing microbial communities using high-throughput sequencing techniques. Bioinformatics tools help in identifying microbial species, their functions, and their impact on human health and the environment.\n",
        "\n",
        "Systems Biology: This integrates multi-omics data (genomics, transcriptomics, proteomics, metabolomics) to model biological systems and predict their behavior under different conditions.\n",
        "\n",
        "Structural Bioinformatics: This involves modeling and analyzing biological macromolecules such as DNA, RNA, and proteins. Computational tools like molecular docking and molecular dynamics simulations help in drug discovery and protein function prediction.\n",
        "\n",
        "Bioinformatics Tools and Techniques\n",
        "\n",
        "Sequence Alignment: Tools like BLAST and Clustal Omega align DNA, RNA, or protein sequences to identify similarities and evolutionary relationships.\n",
        "\n",
        "Genome Assembly and Annotation: Software like SPAdes, AUGUSTUS, and Prokka help in assembling raw sequencing reads and annotating genes.\n",
        "\n",
        "Differential Expression Analysis: Tools like DESeq2 and edgeR analyze RNA-seq data to identify differentially expressed genes.\n",
        "\n",
        "Pathway and Functional Analysis: Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) help in understanding the biological functions of genes.\n",
        "\n",
        "Machine Learning in Bioinformatics: AI and ML techniques are used for predicting disease risk, drug responses, and classifying biological sequences.\n",
        "\n",
        "Applications of Bioinformatics\n",
        "\n",
        "Disease Diagnosis and Treatment: Bioinformatics helps in identifying disease-associated genes, developing diagnostic markers, and designing targeted therapies.\n",
        "\n",
        "Personalized Medicine: Genomic data is used to tailor treatments based on individual genetic profiles.\n",
        "\n",
        "Agricultural Biotechnology: Genetic analysis of crops and livestock improves yield, resistance, and nutritional content.\n",
        "\n",
        "Environmental Studies: Metagenomic studies analyze microbial communities in different ecosystems, aiding in biodiversity conservation and pollution control.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Bioinformatics is a rapidly evolving field that continues to revolutionize biological research. With advancements in computational power and data analytics, bioinformatics will play an increasingly vital role in healthcare, agriculture, and environmental science, paving the way for significant scientific breakthroughs.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "D93i7i56i2VX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\"<UNK\": 0}\n",
        "\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "rHXm9QfIjS-b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Converting Text to Numerical Indices**\n",
        "- This function converts words in a sentence into their respective indices based on the vocabulary."
      ],
      "metadata": {
        "id": "6bQJYdiewbb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(sentence, vocab):\n",
        "  num_sentences = []\n",
        "\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      num_sentences.append(vocab[token])\n",
        "    else:\n",
        "      num_sentences.append(vocab[\"<UNK>\"])\n",
        "\n",
        "  return num_sentences"
      ],
      "metadata": {
        "id": "jg72nTcdkSAG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Creating Training Sequences**\n",
        "- The document is split into sentences.\n",
        "- Each sentence is converted into numerical indices.\n",
        "- Generates sequential training data where each sample consists of a growing sequence of words.\n"
      ],
      "metadata": {
        "id": "WPAhHtZ7wgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = document.split(\"\\n\")\n",
        "numerical_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))"
      ],
      "metadata": {
        "id": "3JXj5CkHjuZN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "\n",
        "for sentence in numerical_sentences:\n",
        "  for i in range(1, len(sentence)):\n",
        "    training_sequence.append(sentence[:i+1])"
      ],
      "metadata": {
        "id": "cmdg-tJKk6PK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Padding Sequences to Uniform Length**\n",
        "- Ensures all sequences have the same length by padding them with zeros at the beginning."
      ],
      "metadata": {
        "id": "bs8uIulxwn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "\n",
        "for seq in training_sequence:\n",
        "  lengths.append(len(seq))\n",
        "\n",
        "max(lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDSk42KpmMdI",
        "outputId": "9b2a9a8d-3786-426c-9352-c076dd6c255b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = []\n",
        "\n",
        "for seq in training_sequence:\n",
        "  padded_training_sequence.append([0] * (max(lengths) - len(seq)) + seq)"
      ],
      "metadata": {
        "id": "bK7yIrM0mVlo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Y5g-My3Rm27Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Splitting Data into Input and Target**\n",
        "- `X` contains all words except the last one in each sequence (input).\n",
        "- `y` contains the last word in each sequence (target output)."
      ],
      "metadata": {
        "id": "3-EwiHgYwz65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_training_sequence[:, :-1]\n",
        "y = padded_training_sequence[:, -1]"
      ],
      "metadata": {
        "id": "o4XluqVTnE_F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Creating a Dataset Class**\n",
        "- This custom dataset class allows efficient data loading for training.\n",
        "- Uses DataLoader to shuffle and batch the data for training."
      ],
      "metadata": {
        "id": "f7I7lyTFw9hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "4mGrUh-_nUfC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WordDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "1ju3TR-KntPX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Defining the LSTM Model**\n",
        "- **Embedding Layer**: Converts word indices into dense vector representations.\n",
        "- **LSTM Layer**: Processes sequences and learns dependencies between words.\n",
        "- **Fully Connected Layer**: Maps the LSTM output to the vocabulary size for predicting the next word."
      ],
      "metadata": {
        "id": "n_JwdBqyxEuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, 100)\n",
        "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "    self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    inter_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "GX1TUVsZnw-O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Training the Model**\n",
        "- **Loss Function**: CrossEntropyLoss for multi-class classification.\n",
        "- **Optimizer**: Adam optimizer for efficient training.\n",
        "- **Training Loop**: Runs for 50 epochs, computing the loss and updating model parameters."
      ],
      "metadata": {
        "id": "mUCE77F6xXUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(len(vocab))\n",
        "model.to(device)\n",
        "\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4Cl5J0P3rpYd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "\n",
        "  for X_batch, y_batch in dataloader:\n",
        "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_batch)\n",
        "    loss = criterion(output, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwG5cyBssBCv",
        "outputId": "efc8506f-3fac-443c-f253-35e3e9ac1d78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 5.3139\n",
            "Epoch: 2, Loss: 3.8702\n",
            "Epoch: 3, Loss: 2.3366\n",
            "Epoch: 4, Loss: 1.1713\n",
            "Epoch: 5, Loss: 0.5555\n",
            "Epoch: 6, Loss: 0.3002\n",
            "Epoch: 7, Loss: 0.1776\n",
            "Epoch: 8, Loss: 0.1285\n",
            "Epoch: 9, Loss: 0.0860\n",
            "Epoch: 10, Loss: 0.0455\n",
            "Epoch: 11, Loss: 0.0306\n",
            "Epoch: 12, Loss: 0.0275\n",
            "Epoch: 13, Loss: 0.0215\n",
            "Epoch: 14, Loss: 0.0199\n",
            "Epoch: 15, Loss: 0.0189\n",
            "Epoch: 16, Loss: 0.0186\n",
            "Epoch: 17, Loss: 0.0147\n",
            "Epoch: 18, Loss: 0.0140\n",
            "Epoch: 19, Loss: 0.0169\n",
            "Epoch: 20, Loss: 0.0174\n",
            "Epoch: 21, Loss: 0.0170\n",
            "Epoch: 22, Loss: 0.0204\n",
            "Epoch: 23, Loss: 0.0191\n",
            "Epoch: 24, Loss: 0.0111\n",
            "Epoch: 25, Loss: 0.0137\n",
            "Epoch: 26, Loss: 0.0117\n",
            "Epoch: 27, Loss: 0.0122\n",
            "Epoch: 28, Loss: 0.0115\n",
            "Epoch: 29, Loss: 0.0102\n",
            "Epoch: 30, Loss: 0.0111\n",
            "Epoch: 31, Loss: 0.0109\n",
            "Epoch: 32, Loss: 0.0108\n",
            "Epoch: 33, Loss: 0.0099\n",
            "Epoch: 34, Loss: 0.0118\n",
            "Epoch: 35, Loss: 0.0112\n",
            "Epoch: 36, Loss: 0.0089\n",
            "Epoch: 37, Loss: 0.0104\n",
            "Epoch: 38, Loss: 0.0103\n",
            "Epoch: 39, Loss: 0.0104\n",
            "Epoch: 40, Loss: 0.0110\n",
            "Epoch: 41, Loss: 0.0116\n",
            "Epoch: 42, Loss: 0.0121\n",
            "Epoch: 43, Loss: 0.0091\n",
            "Epoch: 44, Loss: 0.0092\n",
            "Epoch: 45, Loss: 0.0095\n",
            "Epoch: 46, Loss: 0.0104\n",
            "Epoch: 47, Loss: 0.0155\n",
            "Epoch: 48, Loss: 0.0167\n",
            "Epoch: 49, Loss: 0.0130\n",
            "Epoch: 50, Loss: 0.0097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Generating Text Using the Trained Model**\n",
        "- Takes an input phrase, tokenizes it, and converts it to numerical form.\n",
        "- Passes it through the model to predict the next word.\n",
        "- Selects the word with the highest probability and appends it to the text.\n",
        "- Generates 10 words iteratively using the trained model.\n",
        "- Introduces a short delay to simulate real-time text generation.\n"
      ],
      "metadata": {
        "id": "da7nB-87xgMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, vocab, text):\n",
        "  tokenized = word_tokenize(text.lower())\n",
        "  num_tokenized = text_to_indices(tokenized, vocab)\n",
        "  padded_num_tokenized = torch.tensor([0] * (max(lengths) - len(num_tokenized)) + num_tokenized, dtype=torch.long).unsqueeze(0).to(device)\n",
        "  output = model(padded_num_tokenized)\n",
        "  value, index = torch.max(output, dim=1)\n",
        "\n",
        "  return text + \" \" + list(vocab.keys())[index]"
      ],
      "metadata": {
        "id": "GhaU-5URsuZi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = 10\n",
        "input_text = \"Bioinformatics is an\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "  predicted_text = prediction(model, vocab, input_text)\n",
        "  print(predicted_text)\n",
        "  input_text = predicted_text\n",
        "  time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNhlXM6wuiXq",
        "outputId": "1526cfa2-fad9-4115-b37a-d4755cfaa4ad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bioinformatics is an interdisciplinary\n",
            "Bioinformatics is an interdisciplinary field\n",
            "Bioinformatics is an interdisciplinary field that\n",
            "Bioinformatics is an interdisciplinary field that combines\n",
            "Bioinformatics is an interdisciplinary field that combines biology\n",
            "Bioinformatics is an interdisciplinary field that combines biology ,\n",
            "Bioinformatics is an interdisciplinary field that combines biology , computer\n",
            "Bioinformatics is an interdisciplinary field that combines biology , computer science\n",
            "Bioinformatics is an interdisciplinary field that combines biology , computer science ,\n",
            "Bioinformatics is an interdisciplinary field that combines biology , computer science , mathematics\n"
          ]
        }
      ]
    }
  ]
}