{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding PyTorch Autograd: A Practical Guide to Gradient Calculations**"
      ],
      "metadata": {
        "id": "iSq-APSpYyhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Overview**\n",
        ">This notebook explores PyTorch's autograd feature, focusing on automatic differentiation, gradient calculations, and backward propagation. Each example illustrates key concepts, including manual gradient derivation, using requires_grad, and different ways to manage gradients. This foundational knowledge will help understand deep learning optimization processes."
      ],
      "metadata": {
        "id": "9JC4dYQ9YpYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Importing PyTorch**\n",
        ">Importing the library PyTorch for Autograd."
      ],
      "metadata": {
        "id": "AJJNqCutchUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NFMFVTSpQHtr"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Custom Binary Cross-Entropy Implementation**\n",
        ">This defines a binary cross-entropy loss function to compute the error between predictions and actual labels in a binary classification task. The `torch.clamp` function ensures numerical stability by bounding prediction values within [`epsilon`, `1-epsilon`] to avoid `log(0)` errors. The formula calculates the negative log likelihood for the correct class based on the predicted probabilities."
      ],
      "metadata": {
        "id": "CBjQY5J5Y-8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(prediction, target):\n",
        "  epsilon = 1e-8\n",
        "  prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
        "\n",
        "  return -1*target*torch.log(prediction) - (1-target)*torch.log(1-prediction)"
      ],
      "metadata": {
        "id": "_Ob5Bos5Vd9v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Manual Gradient Computation**\n",
        ">This initializes the input (`x`), target (`y`), weight (`w`), and bias (`b`). Using the linear model equation `z = wx + b`, the weighted sum is computed. The result is passed through the sigmoid function to produce a probability (`prediction`). Finally, the binary cross-entropy loss is calculated using the custom function."
      ],
      "metadata": {
        "id": "XUE_L-5WZRWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.73)\n",
        "y = torch.tensor(1.0)\n",
        "\n",
        "w = torch.tensor(1.0)\n",
        "b = torch.tensor(0.0)\n",
        "\n",
        "z = w*x + b\n",
        "prediction = torch.sigmoid(z)\n",
        "loss = binary_cross_entropy(prediction, y)\n",
        "\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Prediction: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLewPgHpQY-F",
        "outputId": "2f99a887-8297-4759-bbed-d8234e796d63"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.02370944619178772\n",
            "Prediction: 0.976569414138794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">The gradients are calculated manually using the chain rule. Each component of the derivative is computed:\n",
        "  - `dloss_dpred`: Partial derivative of the loss w.r.t. the prediction.\n",
        "  - `dpred_dz`: Derivative of the sigmoid function w.r.t. its input.\n",
        "  - `dz_dw` and `dz_db`: Derivatives of the linear model w.r.t. the weight and bias, respectively.<br>\n",
        "The final gradients for the weight (`dloss_dw`) and bias (`dloss_db`) are computed by combining these components."
      ],
      "metadata": {
        "id": "h_ld1S5TaPur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dloss_dpred = (prediction-y) / (prediction * (1-prediction))\n",
        "dpred_dz = prediction * (1-prediction)\n",
        "\n",
        "dz_dw = x\n",
        "dz_db = 1\n",
        "\n",
        "dloss_dw = dloss_dpred * dpred_dz * dz_dw\n",
        "dloss_db = dloss_dpred * dpred_dz * dz_db\n",
        "\n",
        "print(f\"Gradient of loss with respect to weight: {dloss_dw}\")\n",
        "print(f\"Gradient of loss with respect to bias: {dloss_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT9N4E2gZ7x2",
        "outputId": "4b0c9d2e-7d6d-455f-bcc3-fa652addabe4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of loss with respect to weight: -0.08739608526229858\n",
            "Gradient of loss with respect to bias: -0.023430585861206055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Automatic Differentiation with `requires_grad`**\n",
        ">This cell uses PyTorchâ€™s autograd by setting `requires_grad=True` for `w2` and `b2`. This enables automatic tracking of operations on these tensors. The same forward computation (`z2`, `y_pred`, `loss2`) is performed, and the gradients are computed automatically using `loss2.backward()`. The gradients are stored in the `.grad` attribute of `w2` and `b2`."
      ],
      "metadata": {
        "id": "D2lUd6b4bEh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = torch.tensor(3.73)\n",
        "y2 = torch.tensor(1.0)\n",
        "\n",
        "w2 = torch.tensor(1.0, requires_grad=True)\n",
        "b2 = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "z2 = w2*x2 + b2\n",
        "y_pred = torch.sigmoid(z2)\n",
        "loss2 = binary_cross_entropy(y_pred, y2)\n",
        "\n",
        "print(f\"Loss: {loss2}\")\n",
        "print(f\"Prediction: {y_pred}\")\n",
        "\n",
        "loss2.backward()\n",
        "\n",
        "print(f\"\\n\\nGradient of loss with respect to weight: {w2.grad}\")\n",
        "print(f\"Gradient of loss with respect to bias: {b2.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUjwj4LMTO5x",
        "outputId": "9d3b04a8-a7ae-4eb8-f9df-12ad512d79b6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.02370944619178772\n",
            "Prediction: 0.976569414138794\n",
            "\n",
            "\n",
            "Gradient of loss with respect to weight: -0.08739608526229858\n",
            "Gradient of loss with respect to bias: -0.023430585861206055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Gradient Calculation for a Vector Tensor**\n",
        ">This example computes the gradient for a vector tensor `x3`. The operation squares each element of `x3` and calculates their mean (`y3`). When `y3.backward()` is called, gradients of `y3` w.r.t. each element of `x3` are computed and stored in `x3.grad`. This shows how autograd handles tensor-level gradients."
      ],
      "metadata": {
        "id": "qt5HrbVnbeVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y3 = (x3**2).mean()\n",
        "\n",
        "print(f\"Value of y: {y3}\")\n",
        "\n",
        "y3.backward()\n",
        "\n",
        "print(f\"\\n\\nGradients of y: {x3.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3UBT_yUdKy",
        "outputId": "f1d9ed06-276c-416e-e671-c88aead6c1c1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of y: 4.666666507720947\n",
            "\n",
            "\n",
            "Gradients of y: tensor([0.6667, 1.3333, 2.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Clearing Gradients**\n",
        ">This example shows that gradients persist in `.grad` after calling `.backward()`. The gradient of `y4 = x4**2` is computed, and `x4.grad` stores the result. The `zero_()` method clears the gradients, which is important in iterative training loops to prevent accumulation of gradients from previous iterations."
      ],
      "metadata": {
        "id": "UXBoq8zhbut5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x4 = torch.tensor(2.0, requires_grad=True)\n",
        "y4 = x4**2\n",
        "\n",
        "print(f\"Value of y: {y4}\")\n",
        "\n",
        "y4.backward()\n",
        "\n",
        "print(f\"\\n\\nGradients of y: {x4.grad}\")\n",
        "x4.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugWv7XEoWFCG",
        "outputId": "92c75058-016b-4560-bc7a-91710734e554"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of y: 4.0\n",
            "\n",
            "\n",
            "Gradients of y: 4.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Disabling Gradient Tracking**\n",
        ">This demonstrates two methods to disable gradient tracking:\n",
        "  - `requires_grad_(False)`: Permanently disables gradient computation for the tensor `x6`.\n",
        "  - `detach()`: Creates a new tensor `x7` detached from the computation graph of `x5`. Any operation on `x7` will not affect the gradients of `x5`."
      ],
      "metadata": {
        "id": "ZHVo3YZnb6Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x5 = torch.tensor(2.0, requires_grad=True)\n",
        "y5 = x5**3\n",
        "\n",
        "print(f\"Value of y: {y5}\")\n",
        "\n",
        "y5.backward()\n",
        "\n",
        "print(f\"Gradients of y: {x5.grad}\")\n",
        "\n",
        "x6 = x5.requires_grad_(False)\n",
        "y6 = x6**4\n",
        "\n",
        "print(f\"\\nValue of y (requires_grad_): {y6}\")\n",
        "\n",
        "x7 = x5.detach()\n",
        "y7 = x7**5\n",
        "\n",
        "print(f\"Value of y (detach): {y7}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFVGjMcXWqaY",
        "outputId": "9c17ceb9-0380-43a2-8361-a1b898d6535e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of y: 8.0\n",
            "Gradients of y: 12.0\n",
            "\n",
            "Value of y (requires_grad_): 16.0\n",
            "Value of y (detach): 32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Using no_grad for Inference**\n",
        ">This example shows the use of `torch.no_grad()` to temporarily disable gradient tracking within its scope. This is useful for inference, where gradients are not needed, reducing memory usage and computation overhead."
      ],
      "metadata": {
        "id": "ak8t7tM8cQ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x8 = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y8 = x8**6\n",
        "\n",
        "print(f\"Value of y (no_grad): {y8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbt338RkcP0h",
        "outputId": "24b881a3-f9f4-4822-ff7b-28c319599255"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of y (no_grad): 64.0\n"
          ]
        }
      ]
    }
  ]
}